/*
 * Copyright 2015-2018 Snowflake Computing
 * Copyright 2015 TouchType Ltd
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package net.snowflake.spark.snowflake

import java.net.URI

import com.amazonaws.auth.AWSCredentials
import com.amazonaws.services.s3.AmazonS3Client
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.sources._
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.slf4j.LoggerFactory
import net.snowflake.spark.snowflake.Parameters.MergedParameters
import net.snowflake.spark.snowflake.io.SupportedFormat.SupportedFormat
import net.snowflake.spark.snowflake.io.{SupportedFormat, SupportedSource}
import net.snowflake.spark.snowflake.io.SupportedSource.SupportedSource

import scala.collection.mutable.ArrayBuffer
import scala.reflect.ClassTag

/** Data Source API implementation for Amazon Snowflake database tables */
private[snowflake] case class SnowflakeRelation(
                                                 jdbcWrapper: JDBCWrapper,
                                                 s3ClientFactory: AWSCredentials => AmazonS3Client,
                                                 params: MergedParameters,
                                                 userSchema: Option[StructType])(@transient val sqlContext: SQLContext)
  extends BaseRelation
    with PrunedFilteredScan
    with InsertableRelation {

  import SnowflakeRelation._

  override def toString: String = {
    "SnowflakeRelation"
  }

  val log = LoggerFactory.getLogger(getClass) // Create a temporary stage

  private lazy val creds = CloudCredentialsUtils
    .load(params.rootTempDir, sqlContext.sparkContext.hadoopConfiguration)

  if (sqlContext != null && params.usingExternalStage) {
    Utils.checkFileSystem(
      new URI(params.rootTempDir),
      sqlContext.sparkContext.hadoopConfiguration)
  }

  override lazy val schema: StructType = {
    userSchema.getOrElse {
      val tableNameOrSubquery =
        params.query.map(q => s"($q)").orElse(params.table.map(_.toString)).get
      val conn = jdbcWrapper.getConnector(params)
      try {
        jdbcWrapper.resolveTable(conn, tableNameOrSubquery)
      } finally {
        conn.close()
      }
    }
  }

  override def insert(data: DataFrame, overwrite: Boolean): Unit = {
    val saveMode = if (overwrite) {
      SaveMode.Overwrite
    } else {
      SaveMode.Append
    }
    val writer = new SnowflakeWriter(jdbcWrapper, s3ClientFactory)
    writer.save(sqlContext, data, saveMode, params)
  }

  override def unhandledFilters(filters: Array[Filter]): Array[Filter] = {
    filters.filterNot(filter =>
      FilterPushdown.buildFilterExpression(schema, filter).isDefined)
  }

  // Build the RDD from a query string, generated by SnowflakeStrategy. Type can be InternalRow to comply with
  // SparkPlan's doExecute().
  def buildScanFromSQL[T: ClassTag](sql: String,
                                    schema: Option[StructType]): RDD[T] = {
    if (params.checkBucketConfiguration && params.usingExternalStage) {
      Utils.checkThatBucketHasObjectLifecycleConfiguration(
        params.rootTempDir,
        params.rootTempDirStorageType,
        s3ClientFactory(creds))
    }

    log.debug(Utils.sanitizeQueryText(sql))

    val conn = jdbcWrapper.getConnector(params)
    val resultSchema = schema.getOrElse(try {
      jdbcWrapper.resolveTable(conn, sql)
    } finally {
      conn.close()
    })

    getRDD[T](sql, resultSchema)
  }

  // Build RDD result from PrunedFilteredScan interface. Maintain this here for backwards compatibility and for
  // when extra pushdowns are disabled.
  override def buildScan(requiredColumns: Array[String],
                         filters: Array[Filter]): RDD[Row] = {
    if (params.checkBucketConfiguration && params.usingExternalStage) {
      Utils.checkThatBucketHasObjectLifecycleConfiguration(
        params.rootTempDir,
        params.rootTempDirStorageType,
        s3ClientFactory(creds))
    }
    if (requiredColumns.isEmpty) {
      // In the special case where no columns were requested, issue a `count(*)` against Snowflake
      // rather than unloading data.
      val whereClause = FilterPushdown.buildWhereClause(schema, filters)
      val tableNameOrSubquery =
        params.query.map(q => s"($q)").orElse(params.table).get
      val countQuery =
        s"SELECT count(*) FROM $tableNameOrSubquery $whereClause"
      log.debug(Utils.sanitizeQueryText(countQuery))
      val conn = jdbcWrapper.getConnector(params)
      try {
        val results = jdbcWrapper.executeQueryInterruptibly(conn, countQuery)
        if (results.next()) {
          val numRows = results.getLong(1)
          val parallelism =
            sqlContext.getConf("spark.sql.shuffle.partitions", "200").toInt
          val emptyRow = Row.empty
          sqlContext.sparkContext
            .parallelize(1L to numRows, parallelism)
            .map(_ => emptyRow)
        } else {
          throw new IllegalStateException(
            "Could not read count from Snowflake")
        }
      } finally {
        conn.close()
      }
    } else {
      // Unload data from Snowflake into a temporary directory in S3:
      //  val tempDir = params.createPerQueryTempDir()
      // val unloadSql =
      //     buildUnloadStmt(standardQuery(requiredColumns, filters), tempDir)
      val prunedSchema = pruneSchema(schema, requiredColumns)

      getRDD[Row](standardQuery(requiredColumns, filters), prunedSchema)
    }
  }

  // Get an RDD from an unload statement. Provide result schema because
  // when a custom SQL statement is used, this means that we cannot know the results
  // without first executing it.
  private def getRDD[T: ClassTag](
                                   sql: String,
                                   resultSchema: StructType,
                                   format: SupportedFormat = SupportedFormat.CSV
                                 ): RDD[T] = {
    val source: SupportedSource =
      if (params.usingExternalStage) SupportedSource.S3EXTERNAL
      else SupportedSource.S3INTERNAL

    val rdd: RDD[String] = io.readRDD(sqlContext, params, sql, jdbcWrapper, source, format)

    format match {
      case SupportedFormat.CSV =>
        val converter = Conversions.createRowConverter[T](resultSchema)
        val delimiter = '|'
        val quoteChar = '"'

        rdd.map(s => {
          val fields = ArrayBuffer.empty[String]
          var buff = new StringBuilder

          def addField(): Unit = {
            if (buff.isEmpty) fields.append(null)
            else {
              val field = buff.toString()
              buff = new StringBuilder
              fields.append(field)
            }
          }

          var escaped = false
          var index = 0

          while (index < s.length) {
            escaped = false
            if (s(index) == quoteChar) {
              index += 1
              while (index < s.length && !(escaped && s(index) == delimiter)) {
                if (escaped) {
                  escaped = false
                  buff.append(s(index))
                }
                else if (s(index) == quoteChar) escaped = true
                else buff.append(s(index))
                index += 1
              }
              addField()
            }
            else {
              while (index < s.length && s(index) != delimiter) {
                buff.append(s(index))
                index += 1
              }
              addField()
            }
            index += 1
          }

          addField()

          converter(fields.toArray)
        })
      case SupportedFormat.JSON =>
        //todo
        sqlContext.sparkContext.emptyRDD[T]
    }

  }


  // Build a query out of required columns and filters. (Used by buildScan)
  private def standardQuery(requiredColumns: Array[String],
                            filters: Array[Filter]): String = {
    assert(!requiredColumns.isEmpty)
    // Always quote column names, and uppercase-cast them to make them equivalent to being unquoted
    // (unless already quoted):
    val columnList = requiredColumns
      .map(col => if (isQuoted(col)) col else "\"" + col.toUpperCase + "\"")
      .mkString(", ")
    val whereClause = FilterPushdown.buildWhereClause(schema, filters)
    val tableNameOrSubquery =
      params.query.map(q => s"($q)").orElse(params.table.map(_.toString)).get
    s"SELECT $columnList FROM $tableNameOrSubquery $whereClause"
  }
}

private[snowflake] object SnowflakeRelation {

  private def pruneSchema(schema: StructType,
                          columns: Array[String]): StructType = {
    val fieldMap = Map(schema.fields.map(x => x.name -> x): _*)
    new StructType(columns.map(name => fieldMap(name)))
  }

  private def isQuoted(name: String): Boolean = {
    name.startsWith("\"") && name.endsWith("\"")
  }
}
